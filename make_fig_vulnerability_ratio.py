#!/usr/bin/env python3
"""
Make Figure â€“ Choropleth of Vulnerability Ratio.

The vulnerability ratio is defined as:
(% poorest 20% exposed) / (% total population exposed)
= (poorest_20_affected / poorest_20_population) / (total_affected / total_population)

A ratio > 1 indicates the poorest 20% are disproportionately exposed.
A ratio < 1 indicates the poorest 20% are less exposed than the general population.
A ratio = 1 indicates proportional exposure.

Usage (example):
    python make_fig_vulnerability_ratio.py --ssp SSP2 --year 2020 --rp RP100
"""
import argparse
import os
import geopandas as gpd
import matplotlib
matplotlib.use("Agg")          # headless / SLURM safe
import matplotlib.pyplot as plt
import mapclassify
import numpy as np # Added for np.inf, np.nan

# Set sans-serif font globally for the script
plt.rcParams['font.family'] = 'sans-serif'

# ----------------------------------------------------------------------
# Hard-wired roots (same style as flood_population_analysis_new.py)
BASE_OUT_DIR = "/hdrive/all_users/wiederkehr/analysis/analysis_runs_output_all_combinations"
FIG_DIR      = "/hdrive/all_users/wiederkehr/analysis/bachelor/"        # <-- new folder
os.makedirs(FIG_DIR, exist_ok=True)

def build_paths(ssp: str, year: str, rp: str):
    results_dir = os.path.join(BASE_OUT_DIR, f"results_{ssp}_{year}_{rp}")
    geojson = os.path.join(results_dir, f"analysis_results_{ssp}_{year}_{rp}.geojson")
    # The CSV path is built but not explicitly used in this plotting script, similar to original
    csv     = os.path.join(results_dir, f"detailed_analysis_results_{ssp}_{year}_{rp}.csv")
    return geojson, csv

def plot_vulnerability_ratio(geojson_path: str, ssp: str, year: str, rp: str):
    gdf = gpd.read_file(geojson_path)
    
    required_cols = ['poorest_20_affected', 'poorest_20_population', 'total_affected', 'total_population']
    for col in required_cols:
        if col not in gdf.columns:
            raise ValueError(f"Required column '{col}' not found in GeoJSON.")

    # Calculate shares, handling potential division by zero by yielding NaN
    gdf["share_poorest20_exposed"] = np.where(
        gdf["poorest_20_population"] > 0,
        gdf["poorest_20_affected"] / gdf["poorest_20_population"],
        np.nan
    )
    gdf["share_total_exposed"] = np.where(
        gdf["total_population"] > 0,
        gdf["total_affected"] / gdf["total_population"],
        np.nan
    )

    # Calculate vulnerability ratio
    # Handles cases:
    # - If share_poorest20_exposed is NaN or share_total_exposed is NaN -> ratio is NaN
    # - If share_poorest20_exposed > 0 and share_total_exposed == 0 -> ratio is inf
    # - If share_poorest20_exposed == 0 and share_total_exposed > 0 -> ratio is 0
    # - If share_poorest20_exposed == 0 and share_total_exposed == 0 -> ratio is NaN (0/0)
    gdf["vulnerability_ratio"] = gdf["share_poorest20_exposed"] / gdf["share_total_exposed"]

    # Replace inf/-inf with NaN for robust plotting and statistics
    gdf["vulnerability_ratio"] = gdf["vulnerability_ratio"].replace([np.inf, -np.inf], np.nan)

    data_col_name = "vulnerability_ratio"
    valid_data = gdf[data_col_name].dropna()

    # --- Define plotting scheme and colormap ---
    cmap_to_use = "RdBu_r"  # Blue_White_Red diverging: low ratios blue, high ratios red
    plot_scheme_name = "NaturalBreaks" # Default
    plot_k_val = 5 # Default k for NaturalBreaks
    plot_classification_kwargs = {}
    user_defined_bins_for_plot = None

    if not valid_data.empty and valid_data.nunique() > 1:
        min_val = valid_data.min()
        max_val = valid_data.max()
        
        # Define potential internal split points for the diverging map around 1.0
        # Aiming for classes like: <0.8 | 0.8-0.95 | 0.95-1.05 (neutral) | 1.05-1.2 | >1.2
        potential_splits = [0.8, 0.95, 1.05, 1.2] 
        
        current_bins = [min_val]
        for split_val in potential_splits:
            # Add split if it's within the actual data range and creates a new interval
            if min_val < split_val < max_val and split_val > current_bins[-1]:
                current_bins.append(split_val)
        
        if max_val > current_bins[-1]: # Ensure max_val is greater than the last split/min_val
            current_bins.append(max_val)
        
        # Ensure bins are unique and strictly increasing (final check)
        final_bins_for_scheme = []
        if current_bins:
            final_bins_for_scheme.append(current_bins[0])
            for b_val in current_bins[1:]:
                if b_val > final_bins_for_scheme[-1]: 
                    final_bins_for_scheme.append(b_val)

        if len(final_bins_for_scheme) >= 2: # Need at least 2 bin edges for 1 class
            user_defined_bins_for_plot = final_bins_for_scheme
            plot_scheme_name = "UserDefined"
            plot_classification_kwargs = {"bins": user_defined_bins_for_plot}
            # plot_k_val is not directly used by UserDefined scheme when bins are provided to geopandas.plot

    # --- Generate TXT description ---
    stats_lines = [
        f"   - Total number of regions in dataset: {len(gdf)}",
        f"   - Number of regions with valid '{data_col_name}' data: {valid_data.count()}",
        f"   - Number of regions with no data (NaN/Inf) for '{data_col_name}': {gdf[data_col_name].isnull().sum()}"
    ]
    if not valid_data.empty:
        stats_lines.extend([
            f"   - Minimum ratio: {valid_data.min():.2f} (in regions with valid data)",
            f"   - Maximum ratio: {valid_data.max():.2f} (in regions with valid data)",
            f"   - Mean ratio: {valid_data.mean():.2f}",
            f"   - Median ratio: {valid_data.median():.2f}",
            f"   - Standard Deviation: {valid_data.std():.2f}",
            f"   - 25th Percentile (Q1): {valid_data.quantile(0.25):.2f}",
            f"   - 75th Percentile (Q3): {valid_data.quantile(0.75):.2f}"
        ])
    else:
        stats_lines.append("   - No valid data available for statistical summary.")
    summary_stats_str = "\\\\n".join(stats_lines)

    legend_classes_str = "   - Classification not applicable (e.g., no data or single unique value)."
    classifier_for_legend = None
    classification_scheme_desc_text = "N/A"
    actual_k_classes_for_desc = 0

    if not valid_data.empty and valid_data.nunique() > 0:
        if plot_scheme_name == "UserDefined" and user_defined_bins_for_plot and len(user_defined_bins_for_plot) > 1:
            try:
                # user_defined_bins_for_plot contains the full bin edges [min, b1, b2, ..., max]
                # mapclassify.UserDefined expects the upper limits of classes, so pass bins[1:]
                # This aligns with how geopandas.plot internally uses mapclassify for UserDefined scheme
                classifier_input_bins = user_defined_bins_for_plot[1:]
                
                if not classifier_input_bins: # Should not happen if len(user_defined_bins_for_plot) > 1
                    raise ValueError("UserDefined scheme needs at least one class upper bound.")

                classifier_for_legend = mapclassify.UserDefined(valid_data, bins=classifier_input_bins)
                # Now, classifier_for_legend.k is the number of classes.
                # classifier_for_legend.bins contains the upper bounds of these k classes.
                
                # For the description text, show the full edges used for plotting.
                desc_bins_text_list = ['{:.2f}'.format(b) for b in user_defined_bins_for_plot]
                classification_scheme_desc_text = f"User Defined (edges: {desc_bins_text_list})"
                actual_k_classes_for_desc = classifier_for_legend.k
            except Exception as e: # Fallback if UserDefined somehow fails
                print(f"Warning: UserDefined classification for legend failed: {e}. Falling back to NaturalBreaks.")
                plot_scheme_name = "NaturalBreaks" # Revert for safety
                classifier_for_legend = None 
        
        if plot_scheme_name == "NaturalBreaks": # Handles initial default or fallback
            num_unique_vals = valid_data.nunique()
            if num_unique_vals == 1:
                classification_scheme_desc_text = f"Single unique value ({valid_data.iloc[0]:.2f})"
                actual_k_classes_for_desc = 1
            elif num_unique_vals > 1:
                k_to_use_for_natural_breaks = min(plot_k_val, num_unique_vals)
                if k_to_use_for_natural_breaks < 2: k_to_use_for_natural_breaks = 2 # NaturalBreaks needs k>=2
                try:
                    classifier_for_legend = mapclassify.NaturalBreaks(valid_data, k=k_to_use_for_natural_breaks)
                    classification_scheme_desc_text = f"Natural Breaks (Jenks) - {classifier_for_legend.k} classes"
                    actual_k_classes_for_desc = classifier_for_legend.k
                except Exception as e:
                    legend_classes_str = f"   - Could not determine Natural Breaks classes: {e}"
                    classification_scheme_desc_text = "Natural Breaks (Jenks) - Failed"
                    classifier_for_legend = None
        
        if classifier_for_legend:
            legend_classes = []
            # classifier_for_legend.k is the number of classes.
            # classifier_for_legend.bins contains the k upper bounds of these classes.
            for j in range(classifier_for_legend.k): # j from 0 to k-1
                upper_bound = classifier_for_legend.bins[j] # Upper bound of the current class j
                
                if j == 0:
                    if plot_scheme_name == "UserDefined" and user_defined_bins_for_plot:
                        # The first element of user_defined_bins_for_plot is the absolute min for the first class
                        lower_bound = user_defined_bins_for_plot[0]
                    else: # NaturalBreaks or other schemes
                        # For NaturalBreaks, the first lower bound is effectively valid_data.min()
                        lower_bound = valid_data.min() 
                else:
                    # The lower bound of the current class j is the upper bound of the previous class (j-1)
                    lower_bound = classifier_for_legend.bins[j-1]
                
                count = classifier_for_legend.counts[j]
                
                # Ensure lower_bound is not greater than upper_bound for display (can happen with precision issues or single point classes)
                actual_lower_bound_for_display = min(lower_bound, upper_bound)

                class_info_line = f"   - Class {j+1} (Ratio {actual_lower_bound_for_display:.2f} - {upper_bound:.2f}): {count} regions."

                example_region_indices = valid_data[classifier_for_legend.yb == j].index
                num_examples_to_show = min(3, count)
                if num_examples_to_show > 0:
                    example_regions_details = []
                    for i in range(num_examples_to_show):
                        region_original_index = example_region_indices[i]
                        region_name = gdf.loc[region_original_index].get('region_name', 'N/A')
                        nuts_id = gdf.loc[region_original_index].get('nuts_id', 'N/A')
                        value = gdf.loc[region_original_index, data_col_name]
                        example_regions_details.append(f"{region_name} ({nuts_id}): {value:.2f}")
                    class_info_line += " Examples: " + ", ".join(example_regions_details)
                    if count > num_examples_to_show:
                        class_info_line += ", ..."
                    else:
                        class_info_line += "."
                legend_classes.append(class_info_line)
            legend_classes_str = "\\\\n".join(legend_classes)
        elif valid_data.nunique() == 1 and not valid_data.empty:
             legend_classes_str = f"   - Data has 1 unique value: {valid_data.iloc[0]:.2f}. All regions with data will have this color."
    top_n = 5
    top_regions_lines = [f"   - Top {top_n} regions with highest '{data_col_name}':"]
    if not valid_data.empty:
        gdf_sorted = gdf.sort_values(by=data_col_name, ascending=False).dropna(subset=[data_col_name])
        for i in range(min(top_n, len(gdf_sorted))):
            row = gdf_sorted.iloc[i]
            region_name = row.get('region_name', 'N/A')
            nuts_id = row.get('nuts_id', 'N/A')
            value = row[data_col_name]
            top_regions_lines.append(f"     {i+1}. {region_name} ({nuts_id}): {value:.2f}")
    else:
        top_regions_lines.append("     - No data to determine top regions.")
    top_regions_str = "\\n".join(top_regions_lines)

    # Regions with ratio around 1 (e.g., 0.95 to 1.05)
    ratio_around_one_lines = [f"   - Regions with '{data_col_name}' close to 1 (e.g., 0.95 - 1.05):"]
    if not valid_data.empty:
        around_one_gdf = gdf[
            (gdf[data_col_name] >= 0.95) & (gdf[data_col_name] <= 1.05)
        ].dropna(subset=[data_col_name])
        around_one_count = len(around_one_gdf)
        ratio_around_one_lines.append(f"     - Count: {around_one_count}")
        if 0 < around_one_count <= 5:
            for i in range(around_one_count):
                row = around_one_gdf.iloc[i]
                region_name = row.get('region_name', 'N/A')
                nuts_id = row.get('nuts_id', 'N/A')
                value = row[data_col_name]
                ratio_around_one_lines.append(f"       - {region_name} ({nuts_id}): {value:.2f}")
        elif around_one_count > 5:
            row_example = around_one_gdf.iloc[0]
            ratio_around_one_lines.append(f"       - (Examples: {row_example.get('region_name', 'N/A')} ({row_example.get('nuts_id', 'N/A')}): {row_example[data_col_name]:.2f}, ...)")